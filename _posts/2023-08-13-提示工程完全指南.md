---
layout: post
title: "提示工程完全指南"
categories: ai
---
您可能会想知道为什么在您已经知道ChatGPT如何工作并可以与其进行交流以获取答案的情况下，还需要提示工程。然而，请考虑以下例子：

我让GPT对奇数进行求和，并告诉我结果是偶数还是奇数。不幸的是，它没有给出正确的答案。这个失败的原因在于我使用的提示。您可能会认为我可以轻松为这个特定问题提供更好的提示，但想象一下更大或更复杂的情况，许多人在从ChatGPT中生成解决方案方面都很困难。

在这种情况下，您提供的提示确实很重要。正如您从我对提示的简单改变中可以看到，它未能给出正确的答案。那么，这是否意味着我们不应该使用GPT？不，我们肯定应该使用它，但要正确提示。这就是提示工程发挥作用的地方，它允许我们优化输入并指导GPT生成更准确和期望的输出。

### 什么是提示工程？

想象一下你有一个超级聪明的助手，我们称之为AI助手，它可以回答你提出的任何问题。例如，如果你问它：“法国的首都是哪里？”，它会给出正确的答案：“巴黎”。

现在，假设你想让AI助手更加出色。你希望它不仅能告诉你一个国家的首都，还能提供关于它的简短历史。在提示工程中，你可以微调你的指令来实现这一点。这意味着你不仅仅问“法国的首都是哪里？”，而是重新表述为“告诉我关于法国的首都及其历史意义”。通过调整提示，你引导AI助手给出你想要的结果。

在现实生活中，提示工程在许多应用中被使用。例如，想想Siri或Alexa这样的虚拟助手。当你问他们一个问题时，你提问的方式会影响他们回答的质量。通过理解提示工程，开发人员可以改进这些系统，给我们提供更准确和有帮助的答案。

### LLM参数

在影响LLM（大型语言模型）输出的各种参数中，有两个起到了重要作用：温度和top_p值。让我们分别定义这些参数，以了解它们对生成结果的影响。

温度：把温度想象成烹饪中的辣度。较低的温度值使语言模型更加保守，并坚持最有可能的预测。就像少加些辣料一样，结果更加一致可预测。而较高的温度值则为混合物添加更多的随机性和创造力，就像在菜肴中添加更多的辣味，得到意想不到的风味组合。

top_p值：想象一下，你有一个有多个可能答案的多项选择题。top_p值就像设定一个考虑选项的阈值。较低的top_p值意味着只会选择最有可能的答案，保持事物的聚焦和精确。就像只考虑前几个选择一样。相反，较高的top_p值扩大了选项范围，包括更多的可能性和多样化的回答。

简而言之，温度影响语言模型输出中的随机程度，而top_p值控制所考虑选择的范围。

### 基本提示

让我们通过一个非常简单的提示来进行实验：

当你以“天空是”开头的句子时，它会给你提供不同的选择，而不仅仅是一个明确的答案。但是，如果你在句子中包含更重要的细节，你就增加了得到清晰准确答案的机会。让我们看一下另一个例子，通过向提示添加关键信息，可以产生不同的结果。

变清楚了吗？你让模型完成了这个句子，结果得到了更准确的回应，与你的提示相一致。这种构建有效提示来引导模型任务的技术被称为提示工程。

### 提示格式化

简单来说，问题应该格式化为：

而指示应该格式化为：

在格式化问答时，通常采用问题回答（QA）格式，该格式在许多QA数据集中被广泛使用。

上面提到的格式通常被称为零样本提示。之所以称为“零样本”，是因为它不需要提供任何特定的示例或演示来说明问题和答案的结构。

在我之前提到的格式中，还有一种被广泛使用且有效的技术称为少样本提示。在少样本提示中，你可以包含示例来引导模型。下面是如何格式化少样本提示：

问答格式的版本看起来是这样的：

为了更清楚地说明少样本提示的工作原理，这里有一个小分类的例子：

提供的格式非常清晰。每个评论后面跟着两个正斜杠（//），然后是情感值，可以是积极的或消极的。

少样本提示允许语言模型通过提供一些示例来学习任务，这有助于它们理解上下文并表现更好。

### 提示的要素

提示可以包括不同的要素：

指令 - 这是一个具体的任务或指导模型的方向。
上下文 - 它提供额外的信息或背景，以帮助模型生成更好的回应。
输入数据 - 它指的是问题或输入，我们希望模型提供一个回应。
输出指示器 - 它指示所需的输出类型或格式。
并不是所有四个要素在提示中都是必要的，格式取决于特定的任务。

设计提示的一般提示
从简单开始 - 在设计提示时，从简单起步并尝试或迭代以实现最佳结果是很重要的。正如官方指南中提到的，建议从一些基本的平台开始，比如OpenAI或Cohere。你可以逐渐通过添加更多要素和上下文来改进你的提示，以提高结果。在整个过程中，关键是迭代你的提示。

为了设计简单任务的有效提示，使用指令性命令，如“写”，“分类”，“总结”，“翻译”，“排序”等。尝试不同的方法是找到最好的方法的关键。一些建议包括将指令放在提示的开头，并使用清晰的分隔符（如“###”）来区分指令和上下文。

例如：

在你的指令和所需任务中要非常具体和详细。专注于一个结构良好且描述详细的提示。在提示中包含示例可以非常有效。考虑到提示的长度，因为其大小有限制。在具体性和避免不必要细节之间努力取得平衡。

考虑以下这个例子，我们想从一段文本中提取信息：

虽然重视详细信息并改进提示的格式很重要，但避免过于复杂和含糊的样重要。明确和直接通常更有效，就像有效沟通一样。

以下是一个我试图解释的快速例子！假设你想了解提示工程。最初，你可能向ChatGPT简要解释而没有太多细节。你可以尝试这样做：

然而，那个提示可能没有明确指示有多少句子或样式。虽然你可能仍然会得到不错的回应，但更好的方法是（非常具体，简洁明了）：

在设计提示时，与其指定不应该做什么，不如明确指示模型应该做什么。

让我们看一个电影推荐聊天机器人的例子，因为指令的写法不符合期望，所以它没有达到预期的效果。用户要求避免做某件具体的事情，这导致聊天机器人关注错误的事情，而不是用户实际想要它做的事情。

现在，不要告诉机器人不要做什么，让我们明确指导机器人我们希望它做什么。

### 提示工程的应用案例

在本节中，我们将探讨提示工程在不同领域中的各种应用案例，如文本摘要、问答等。

文本摘要
是自然语言生成中常见的任务。我们可以尝试使用提示来完成一个简单的摘要任务。下面的例子将抗生素信息总结为一句话。

### 信息提取

现在，我们将利用语言模型进行信息提取。这涉及从给定的段落中提取相关信息。

红色箭头突出显示了从段落中提取的所需信息。

### 问答

这是一个使用语言模型（LLM）进行问答的指南。

### 文本分类
为了获得所需的标签格式，例如“中性”而不是“Neutral”，在提示中提供具体的指令以获得更好的结果。

例如：

当您提供一个样例来指导模型如何返回情感值时，它将以与提供的样例相同的格式返回该值。

让我们尝试上面的例子，但稍作修改：

模型返回的是“中性”而不是“nutral”，因为提示中没有提供特定的示例来指导所需的输出格式。在提示工程中，明确并提供清晰的示例是非常重要的，以确保模型理解期望的内容。

提示工程允许您指导LLM系统充当对话系统（如聊天机器人等）。这就是角色提示发挥作用的地方。

为了使机器人具有较少的技术性并更易于理解，请在提示中提供额外信息，例如指定回复应以一个七年级学生能理解的语言表示。这将指导机器人使用更简单的语言并避免过多的技术术语。

### 代码生成

确实是语言模型（LLMs）的重要应用案例。GitHub Copilot就是一个利用LLMs生成代码的例子。

本例没有为答案指定编程语言。这突显了即使在提示中缺少一个细节，也会对理解和回答的准确性产生重大影响。

对于语言模型（LLMs）来说，推理是最具挑战性的任务之一，它涉及根据给定信息进行逻辑思考和得出结论的能力。

以下是一个挑战我们LLM理解能力的复杂提示：

作者明确表示他们进行了多次尝试以实现这一目标。这强调了推理确实是使用LLM时面临的最具挑战性的方面之一。

### 提示技术

#### 零-shot提示

像GPT-3这样的大型LLM可以在没有显式训练的情况下完成任务，这要归功于它们遵循指示的能力和在大规模数据集上的广泛训练。这被称为“零-shot”学习。由于它们已经熟悉提示中的单词，因此在这个过程中几乎没有额外的学习需求。

让我们回想一下我们的情感提示示例：

尽管没有明确提到“情感”这个词，但模型的零-shot能力使其能够理解并生成与情感相关的响应，因为它在大规模数据集上进行了训练。它可以根据其现有的知识和上下文推断出这个概念。

#### 少-shot提示

当零-shot方法无法奏效时，我们使用少-shot提示，给予模型一些示例。一个示例意味着一次提示，给两个示例意味着两次提示，依此类推。

一个示例：

首先，我们向模型提供了单词“whatpu”的定义。然后，在要求模型在一个句子中使用“whatpu”之前，我们给出了一个包含这个单词的示例句子。

如果我们回想一下，在先前的推理提示中，我们要求模型添加奇数并确定结果是否为偶数。现在，让我们尝试使用少-shot方法解决相同的问题。

不幸的是，少-shot提示方法并没有为这个推理问题产生可靠的响应。似乎在这种情况下需要使用其他技术或方法来取得更准确和一致的结果。

#### 思维链提示

思维链提示（CoT提示）与少-shot提示一起使用，增强了模型在复杂任务上的推理能力。它将问题分解为较小的步骤，使模型能够在提供响应之前通过中间阶段进行推理。这种组合对于解决需要推理的具有挑战性任务可以取得更好的结果。

将复杂问题分解为子问题有助于LLM提供对复杂问题的准确和适当的响应。它允许模型逐个推理每个子问题，从而对整个问题有更全面的理解，并生成更可靠的答案。

让我们尝试使用思维链提示来解决添加奇数的任务：

这次答案是正确的，这是因为在解决问题时提供了推理步骤。

通过将零-shot提示与思维链提示（CoT提示）结合起来，我们可以通过鼓励模型逐步思考来解决问题。

与其他方法相比，将零-shot提示与少-shot提示和思维链提示（CoT提示）相结合在解决单词问题时显示出更高的性能。通过结合这两种技术，该模型能够逐步推理并生成准确的响应，即使在具有挑战性的问题解决场景中也是如此。

### 自一致性

自一致性是一种高级的提示工程技术，有助于改善少-shot思维链提示（CoT提示）的性能。它涉及使用CoT提示生成多个响应，并选择最一致的答案。对于涉及算术和常识推理的任务，该技术特别有用，因为它提高了模型响应的准确性。

这是一个非常简单的算术任务的例子：

答案是错误的，通过在提示工程中使用自一致性技术，我们可以提高模型在回答特定问题方面的性能。

在思维链提示（CoT提示）的背景下，我们提出多个问题并为每个问题提供答案。最后一个问题就是我们需要一个答案的问题。通过应用这种方法，我们可以引导模型生成更准确和一致的响应。

这里有多个输出：

我们使用自一致性技术计算最终答案，需要进行额外的步骤。有关过程的更详细信息，您可以参考题为“自一致性训练用于组合推理”的论文，该论文位于以下链接：https://arxiv.org/pdf/2203.11171.pdf。该论文提供了关于在组合推理任务的上下文中有效应用自一致性训练的深入见解和技术。

### 生成式知识提示

提示工程中的一种常见技术是结合知识或信息来增强模型的预测准确性。通过提供与所涉及任务相关的相关知识或信息，模型可以利用这些附加的背景来进行更准确的预测。该技术使模型能够利用外部资源或现有知识以提高其理解能力并生成更明智的响应。

这是为什么知识重要的一个例子：

这个错误表明，在需要对世界有更深入理解的任务上，LLM存在一些限制。

让我们不仅回答问题，而且在回答的过程中传授一些知识。

例如，主动提示是一种动态引导语言模型朝着期望的输出方向发展的技术。定向刺激提示探索了将模型引导到特定方向的方法。ReAct侧重于通过积极学习和迭代反馈来改善模型性能。这些只是众多令人兴奋的领域中的几个示例。

提示工程包含多种应用，例如生成数据、生成代码和研究生工作分类案例研究。您还可以深入了解不同的模型架构，如Flan、ChatGPT、LLaMA，甚至了解备受期待的GPT-4。此外，了解与提示工程相关的风险和潜在滥用（如对抗性提示、事实性问题和模型中的偏见）也非常重要。

为了扩展您的知识，您可以探索与提示工程相关的论文、工具、笔记本和数据集。这些资源将为您提供有价值的见解和额外的阅读材料，进一步提高您对这个迷人领域的理解。因此，抓住官方指南，踏上对提示工程广阔可能性的探索之旅！
